As you were experimenting with different neural network architectures, some of you may have trained models that entered into terminal states like this one has. Note both the loss curve as well as the output. What did you do to fix them? And what's going on here? While you may have changed your network architecture, oftentimes, you can fix problems like this simply by retraining your model. Remember, there are still parts of the model training process that are not controlled, such as the random seeds of your weight initializers. The problem in this case is that we seem to have found a position on our loss surface that is small, relative to its neighbors, but nevertheless, much bigger than zero. In other words, we found a local minimum. Note how the loss over time graph actually reached a lower loss value earlier on in the search. The existence and seductiveness of suboptimal local minima are two examples of the shortcomings of our current approach. Others include problems like long training times and the existence of trivial but inappropriate minima. These problems don't have a single cause, and so our methods for dealing with them are diverse. Advanced optimization techniques aim to improve training time and help models not to be seduced by local minima. We'll review some of these later in the course. Data weighting and oversampling and synthetic data creation aim to remove inappropriate minima from the search space altogether. Performance metrics, which is what we'll cover in the next section, tackle the problem at a higher level. Rather than changing the way we search or the search space itself, performance metrics change the way we think about the results of our search by aligning them more closely with what we actually care about. In so doing, they allow us to make better informed decisions about when to search again.