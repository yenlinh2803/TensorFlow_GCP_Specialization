When I introduced a feature cross on this problem, I started off by moving the two axis so that the origin wasn't the center. Now why did they do that? At the time I kind of waved it off by saying that it was just a linear transformation. Just subtracting a constant value from x_1 and x_2 no big deal. Well, I wasn't telling you the whole story. Let's see what happens if I don't move the axis into the center of the diagram. Now, what happens to x_3? The product of x_1 and x_2. Notice that the value of x_3 is small for some blue dots and large for other blue dots. So, if you think just in terms of the values of x_3, their feature cross, you have two linear separation boundaries. To make it just one, you have to translate x_1 by some number, and x_2 by some other number, before you are left with a linear decision boundary on just x_3. These numbers by which you have to translate x_1 and x_2, they are like the weights and bias, more free parameters that your model has to learn. I needed the white lines in just the right position, before I could say that x_3 alone was enough to separate the space. The white lines here are helping to discretize the input space. In this case I have two lines, so they separate the space into four quadrants. So let's follow this idea further. What if we have a more complex problem? Well here is a more complex problem. Obviously a linear model won't help. Or can it?